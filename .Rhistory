p2<-1-p1
##### the new setting error ###############
new.re<-(1-sqrt(5))/2*(model.resid)*p1+(1+sqrt(5))/2*(model.resid)*p2
y.star <- yhat + new.re
return(y.star)
}
J.boot <- as.numeric()
for (b in 1:boot){
cat(paste(b," of ",boot,sep=""),"\r")
y.star <- boot.wild(uhat.a.star)
resid.a <- residuals(add.m(y.star,x,z,h.x,h.z))
sum.sqr.a<-sum(resid.a^2)
npm<-loess(y.star~x+z)
resid.np<-residuals(npm)
sum.sqr.np<-sum(resid.np^2)
J.boot[b] <- (sum.sqr.a-sum.sqr.np)/sum.sqr.np
flush.console()
}
###### the definition of the p.value here ###########
p.value<-length(subset(J.boot,J.boot>J))/boot
return(list(J=J,J.boot=J.boot,p.value=p.value,boot.number=boot))
}
#### the npparametric model test ###########
#### data part #################################
np.fun <- function(x,z,alpha){return(x^2+alpha*x*z+exp(z))}
xfun<-function(x){ return(x^2)}
gfun<-function(x,y,alpha){return(exp(x)+alpha*x*exp(y))}
alpha<-c(0,0.6,1.2,2.4,4,6)
trial <- 10
pvalue<-matrix(0,trial,length(alpha))
## Loop over number of times to conduct test
for (j in 1:trial){
## Loop over alpha values (size vs. power)
for(i in 1:length(alpha)){
n=50
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
m.x<-xfun(x)
m.z<-gfun(z,x,alpha[i])
m.x<-m.x-mean(m.x)
m.z<-m.z-mean(m.z)
y <- np.fun(x,y,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-100
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
pvalue[j,i]<-np.test$p.value
}
}
npm<-loess(y~x+z)
gamm<-add.m(y,x,z,h.x,h.z)
uhat.a <-residuals(gamm)
sum.sqr.a<-sum(uhat.a^2)
yhat<-gamm$gamm1.ohat+gamm$gamm2.ohat
mean.uhat.a<-mean(uhat.a)
uhat.a.star<-uhat.a-mean.uhat.a
#### alternative hypothesis: the normal nonparametric model ####
npm<-loess(y~x+z)
resid.np<-resid(npm)
sum.sqr.np<-sum(resid.np^2)
J<-(sum.sqr.a-sum.sqr.np)/sum.sqr.np
i
i=1
n=50
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
m.x<-xfun(x)
m.z<-gfun(z,x,alpha[i])
m.x<-m.x-mean(m.x)
m.z<-m.z-mean(m.z)
y <- np.fun(x,y,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-100
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
pvalue[j,i]<-np.test$p.value
npm<-loess(y~x+z)
n=50
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
m.x<-xfun(x)
m.z<-gfun(z,x,alpha[i])
m.x<-m.x-mean(m.x)
m.z<-m.z-mean(m.z)
y <- np.fun(x,y,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-100
uhat.a <-residuals(gamm)
sum.sqr.a<-sum(uhat.a^2)
yhat<-gamm$gamm1.ohat+gamm$gamm2.ohat
mean.uhat.a<-mean(uhat.a)
uhat.a.star<-uhat.a-mean.uhat.a
npm<-loess(y~x+z)
npm<-loess(y~x+z,span=1)
npm<-loess(y~x+z,span=1,degree=2)
y
n=50
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
m.x<-xfun(x)
m.z<-gfun(z,x,alpha[i])
m.x<-m.x-mean(m.x)
m.z<-m.z-mean(m.z)
y <- np.fun(x,y,alpha[i])+e
y
n=50
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,y,alpha[i])+e
r
e
y
n=50
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
y
boolnp<-function(y,x,z,h.x,h.z,boot=100)
{
### null hypothesis: the additive model is right ####
### tge gamm isgamm<-add.m(y,x,z,h.x,h.z)
uhat.a <-residuals(gamm)
sum.sqr.a<-sum(uhat.a^2)
yhat<-gamm$gamm1.ohat+gamm$gamm2.ohat
mean.uhat.a<-mean(uhat.a)
uhat.a.star<-uhat.a-mean.uhat.a
#### alternative hypothesis: the normal nonparametric model ####
npm<-loess(y~x+z,span=1,degree=2)
resid.np<-resid(npm)
sum.sqr.np<-sum(resid.np^2)
J<-(sum.sqr.a-sum.sqr.np)/sum.sqr.np the efficient result of the regression ###
a <- -0.6180339887499
P.a <- 0.72360679774998
b <- 1.6180339887499
############# struct the new data sample set###########
## Wild resampling from parametric errors.
boot.wild <- function(model.resid) {
p1<-rbinom(n, 1, prob=(1+sqrt(5))/(2*sqrt(5)))
p2<-1-p1
##### the new setting error ###############
new.re<-(1-sqrt(5))/2*(model.resid)*p1+(1+sqrt(5))/2*(model.resid)*p2
y.star <- yhat + new.re
return(y.star)
}
J.boot <- as.numeric()
for (b in 1:boot){
cat(paste(b," of ",boot,sep=""),"\r")
y.star <- boot.wild(uhat.a.star)
resid.a <- residuals(add.m(y.star,x,z,h.x,h.z))
sum.sqr.a<-sum(resid.a^2)
npm<-loess(y.star~x+z)
resid.np<-residuals(npm)
sum.sqr.np<-sum(resid.np^2)
J.boot[b] <- (sum.sqr.a-sum.sqr.np)/sum.sqr.np
flush.console()
}
###### the definition of the p.value here ###########
p.value<-length(subset(J.boot,J.boot>J))/boot
return(list(J=J,J.boot=J.boot,p.value=p.value,boot.number=boot))
}
#### the npparametric model test ###########
#### data part #################################
np.fun <- function(x,z,alpha){return(x^2+alpha*x*z+exp(z))}
xfun<-function(x){ return(x^2)}
gfun<-function(x,y,alpha){return(exp(x)+alpha*x*exp(y))}
alpha<-c(0,0.6,1.2,2.4,4,6)
trial <- 10
pvalue<-matrix(0,trial,length(alpha))
## Loop over number of times to conduct test
for (j in 1:trial){
## Loop over alpha values (size vs. power)
for(i in 1:length(alpha)){
n=50
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-100
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
pvalue[j,i]<-np.test$p.value
}
}
View(pvalue)
colMeans(pvalue)
alpha<-c(0,0.6,1.2,2.4,4,6)
trial <- 10
pvalue<-matrix(0,trial,length(alpha))
## Loop over number of times to conduct test
for (j in 1:trial){
## Loop over alpha values (size vs. power)
for(i in 1:length(alpha)){
n=100
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-100
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
pvalue[j,i]<-np.test$p.value
}
}
View(pvalue)
colMeans(pvalue)
alpha<-c(0,0.2,0.4,0.6,1.2,2.4,4,6)
trial <- 10
pvalue<-matrix(0,trial,length(alpha))
## Loop over number of times to conduct test
for (j in 1:trial){
## Loop over alpha values (size vs. power)
for(i in 1:length(alpha)){
n=100
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-100
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
pvalue[j,i]<-np.test$p.value
}
}
View(pvalue)
colMeans(pvalue)
alpha<-c(0,0.2,0.4,0.6,1.2,2.4,4,6)
trial <- 20
pvalue<-matrix(0,trial,length(alpha))
## Loop over number of times to conduct test
for (j in 1:trial){
## Loop over alpha values (size vs. power)
for(i in 1:length(alpha)){
n=100
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-200
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
pvalue[j,i]<-np.test$p.value
}
}
View(pvalue)
colMeans(pvalue)
write.csv(cbind(alpha,pvalue),file="C:/Users/JZhou2/Desktop/modal regression method/additive form model/pvalue.linear.csv")
ss<-cbind(alpha,pvalue)
write.csv(cbind(t(alpha),pvalue),file="C:/Users/JZhou2/Desktop/modal regression method/additive form model/pvalue.linear.csv")
ss<-cbind(t(alpha),pvalue)
ss<-cbind(alpha,pvalue)
write.csv(pvalue,file="C:/Users/JZhou2/Desktop/modal regression method/additive form model/pvalue.linear.csv")
c<-c(alpha=0,alpha=0.2,alpha=0.4,alpha=0.6,alpha=1.2,alpha=2.4,alpha=4,6)
write.csv(pvalue,file="C:/Users/JZhou2/Desktop/modal regression method/additive form model/pvalue.linear.csv",row.names = c)
write.csv(pvalue,file="C:/Users/JZhou2/Desktop/modal regression method/additive form model/pvalue.linear.csv")
write.csv(pvalue,file="C:/Users/JZhou2/Desktop/modal regression method/additive form model/pvalue.linearc.csv",row.names = c)
npm<-npreg(y~x+z,bws=c(h.x,h.z),regtype="ll",bandwidth.compute
=FALSE)
npm
npm<-npregbw(y~x+z,bws=c(h.x,h.z),regtype="ll",bandwidth.compute
=FALSE)
npm
boolnp<-function(y,x,z,h.x,h.z,boot=100)
{
### null hypothesis: the additive model is right ####
### tge gamm isgamm<-add.m(y,x,z,h.x,h.z)
uhat.a <-residuals(gamm)
sum.sqr.a<-sum(uhat.a^2)
yhat<-gamm$gamm1.ohat+gamm$gamm2.ohat
mean.uhat.a<-mean(uhat.a)
uhat.a.star<-uhat.a-mean.uhat.a
#### alternative hypothesis: the normal nonparametric model ####
npm<-npreg(y~x+z,bws=c(h.x,h.z),regtype="ll",bandwidth.compute
=FALSE)
resid.np<-resid(npm)
sum.sqr.np<-sum(resid.np^2)
J<-(sum.sqr.a-sum.sqr.np)/sum.sqr.np
##the efficient result of the regression ###
a <- -0.6180339887499
P.a <- 0.72360679774998
b <- 1.6180339887499
############# struct the new data sample set###########
## Wild resampling from parametric errors.
boot.wild <- function(model.resid) {
p1<-rbinom(n, 1, prob=(1+sqrt(5))/(2*sqrt(5)))
p2<-1-p1
##### the new setting error ###############
new.re<-(1-sqrt(5))/2*(model.resid)*p1+(1+sqrt(5))/2*(model.resid)*p2
y.star <- yhat + new.re
return(y.star)
}
J.boot <- as.numeric()
for (b in 1:boot){
cat(paste(b," of ",boot,sep=""),"\r")
y.star <- boot.wild(uhat.a.star)
resid.a <- residuals(add.m(y.star,x,z,h.x,h.z))
sum.sqr.a<-sum(resid.a^2)
npm<-npreg(y.star~x+z,bws=c(h.x,h.z),regtype="ll",bandwidth.compute
=FALSE)
resid.np<-residuals(npm)
sum.sqr.np<-sum(resid.np^2)
J.boot[b] <- (sum.sqr.a-sum.sqr.np)/sum.sqr.np
flush.console()
}
###### the definition of the p.value here ###########
p.value<-length(subset(J.boot,J.boot>J))/boot
return(list(J=J,J.boot=J.boot,p.value=p.value,boot.number=boot))
}
#### the npparametric model test ###########
i=1
n<-100
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-200
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
pvalue[j,i]<-np.test$p.value
n<-100
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-200
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
boolnp<-function(y,x,z,h.x,h.z,boot=100)
{
### null hypothesis: the additive model is right ####
gamm<-add.m(y,x,z,h.x,h.z)
uhat.a <-residuals(gamm)
sum.sqr.a<-sum(uhat.a^2)
yhat<-gamm$gamm1.ohat+gamm$gamm2.ohat
mean.uhat.a<-mean(uhat.a)
uhat.a.star<-uhat.a-mean.uhat.a
#### alternative hypothesis: the normal nonparametric model ####
npm<-npreg(y~x+z,bws=c(h.x,h.z),regtype="ll",bandwidth.compute
=FALSE)
resid.np<-resid(npm)
sum.sqr.np<-sum(resid.np^2)
J<-(sum.sqr.a-sum.sqr.np)/sum.sqr.np
##the efficient result of the regression ###
a <- -0.6180339887499
P.a <- 0.72360679774998
b <- 1.6180339887499
############# struct the new data sample set###########
## Wild resampling from parametric errors.
boot.wild <- function(model.resid) {
p1<-rbinom(n, 1, prob=(1+sqrt(5))/(2*sqrt(5)))
p2<-1-p1
##### the new setting error ###############
new.re<-(1-sqrt(5))/2*(model.resid)*p1+(1+sqrt(5))/2*(model.resid)*p2
y.star <- yhat + new.re
return(y.star)
}
J.boot <- as.numeric()
for (b in 1:boot){
cat(paste(b," of ",boot,sep=""),"\r")
y.star <- boot.wild(uhat.a.star)
resid.a <- residuals(add.m(y.star,x,z,h.x,h.z))
sum.sqr.a<-sum(resid.a^2)
npm<-npreg(y.star~x+z,bws=c(h.x,h.z),regtype="ll",bandwidth.compute
=FALSE)
resid.np<-residuals(npm)
sum.sqr.np<-sum(resid.np^2)
J.boot[b] <- (sum.sqr.a-sum.sqr.np)/sum.sqr.np
flush.console()
}
###### the definition of the p.value here ###########
p.value<-length(subset(J.boot,J.boot>J))/boot
return(list(J=J,J.boot=J.boot,p.value=p.value,boot.number=boot))
}
#### the npparametric model t
boolnp<-function(y,x,z,h.x,h.z,boot=100)
{
### null hypothesis: the additive model is right ####
gamm<-add.m(y,x,z,h.x,h.z)
uhat.a <-residuals(gamm)
sum.sqr.a<-sum(uhat.a^2)
yhat<-gamm$gamm1.ohat+gamm$gamm2.ohat
mean.uhat.a<-mean(uhat.a)
uhat.a.star<-uhat.a-mean.uhat.a
#### alternative hypothesis: the normal nonparametric model ####
npm<-npreg(y~x+z,bws=c(h.x,h.z),regtype="ll",bandwidth.compute
=FALSE)
resid.np<-resid(npm)
sum.sqr.np<-sum(resid.np^2)
J<-(sum.sqr.a-sum.sqr.np)/sum.sqr.np
##the efficient result of the regression ###
a <- -0.6180339887499
P.a <- 0.72360679774998
b <- 1.6180339887499
############# struct the new data sample set###########
## Wild resampling from parametric errors.
boot.wild <- function(model.resid) {
p1<-rbinom(n, 1, prob=(1+sqrt(5))/(2*sqrt(5)))
p2<-1-p1
##### the new setting error ###############
new.re<-(1-sqrt(5))/2*(model.resid)*p1+(1+sqrt(5))/2*(model.resid)*p2
y.star <- yhat + new.re
return(y.star)
}
J.boot <- as.numeric()
for (b in 1:boot){
cat(paste(b," of ",boot,sep=""),"\r")
y.star <- boot.wild(uhat.a.star)
resid.a <- residuals(add.m(y.star,x,z,h.x,h.z))
sum.sqr.a<-sum(resid.a^2)
npm<-npreg(y.star~x+z,bws=c(h.x,h.z),regtype="ll",bandwidth.compute
=FALSE)
resid.np<-residuals(npm)
sum.sqr.np<-sum(resid.np^2)
J.boot[b] <- (sum.sqr.a-sum.sqr.np)/sum.sqr.np
flush.console()
}
###### the definition of the p.value here ###########
p.value<-length(subset(J.boot,J.boot>J))/boot
return(list(J=J,J.boot=J.boot,p.value=p.value,boot.number=boot))
}
#### the npparametric model t
np.fun <- function(x,z,alpha){return(x^2+alpha*x*z+exp(z))}
alpha<-c(0,0.2,0.4,0.6,1.2,2.4,4,6)
trial <- 20
pvalue<-matrix(0,trial,length(alpha))
i=1
n<-100
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-200
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
np.test$p.value
np.fun <- function(x,z,alpha){return(x^2+alpha*x*z+exp(z))}
alpha<-c(0,0.2,0.4,0.6,1.2,2.4,4,6)
trial <- 20
pvalue<-matrix(0,trial,length(alpha))
## Loop over number of times to conduct test
for (j in 1:trial){
## Loop over alpha values (size vs. power)
for(i in 1:length(alpha)){
n<-100
x <- rnorm(n)
z <- rnorm(n)
e <- rnorm(n,sd=0.3)
y <- np.fun(x,z,alpha[i])+e
h.x<-0.5*sd(x)*n^(-1/5)
h.z<-0.5*sd(z)*n^(-1/5)
boot<-200
np.test<-boolnp(y,x,z,h.x,h.z,boot=boot)
pvalue[j,i]<-np.test$p.value
}
}
nunp<-apply(pvalue,2,length(pvalue<0.05)/trial)
pavlue
pvalue
nunp<-apply(pvalue,2,length(pvalue<0.05)/trial)
nunp<-apply(pvalue,2,function(pvalue) length(pvalue<0.05)/trial)
nunp
length(pvalue[,2]<0.05)/trial
length(pvalue[,2]<0.05)
pvalue[,2]
length(pvalue,pvalue<0.05)
length(pvalue[,2],pvalue[,2<0.05)
length(pvalue[,2],pvalue[,2]<0.05)
nunp<-apply(pvalue,2,length(which(pvalue<0.05))/trial)
nunp<-apply(pvalue,2,function(pvalue)length(which(pvalue<0.05))/trial)
nunp
num.sims=20
for (j in 1:length(a.seq)){
nominal1[j] <- mean(ifelse(pvalue[,1]<=a.seq[j],1,0))
se.nominal1[j] <- sqrt(a.seq[j]*(1-a.seq[j])/num.sims)
nominal2[j] <- mean(ifelse(pvalue[,2]<=a.seq[j],1,0))
se.nominal2[j] <- sqrt(a.seq[j]*(1-a.seq[j])/num.sims)
}
a.seq <- seq(0,1,by=0.01)
nominal1 <- as.numeric()
se.nominal1 <- as.numeric()
nominal2 <- as.numeric()
se.nominal2 <- as.numeric()
num.sims=20
for (j in 1:length(a.seq)){
nominal1[j] <- mean(ifelse(pvalue[,1]<=a.seq[j],1,0))
se.nominal1[j] <- sqrt(a.seq[j]*(1-a.seq[j])/num.sims)
nominal2[j] <- mean(ifelse(pvalue[,2]<=a.seq[j],1,0))
se.nominal2[j] <- sqrt(a.seq[j]*(1-a.seq[j])/num.sims)
}
pdf(file="Level_Accuracy_GV.pdf")
plot(a.seq,a.seq,type="l",lwd=2,col="red",lty=1,
xlab="Nominal Level",ylab="Actual Level",
main="Level Accuracy")
lines(a.seq,nominal1,lwd=2,col="blue",lty=2)
plot(a.seq,a.seq,type="l",lwd=2,col="red",lty=1,
xlab="Nominal Level",ylab="Actual Level",
main="Level Accuracy")
lines(a.seq,nominal1,lwd=2,col="blue",lty=2)
plot(x,y)
